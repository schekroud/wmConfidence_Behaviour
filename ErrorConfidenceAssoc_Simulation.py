# -*- coding: utf-8 -*-
"""
Created on Wed Aug 16 10:00:24 2023

@author: sammi
"""

import numpy as np
import scipy as sp
import pandas as pd
import statsmodels.api as sma
import statsmodels as sm
import sklearn as skl
import os.path as op
import os
import sys
from matplotlib import pyplot as plt
import seaborn as sns
import pycircstat as circstat
import progressbar
from numpy import matlib

np.set_printoptions(suppress=True)
%matplotlib

sys.path.insert(0, 'C:/Users/sammirc/Desktop/phd/pyCircMixModel')
import CircMixModel as cmm

def wrap(x):
    return (x+180)%360 - 180
def wrap90(x):
    return (x+90)%180 - 90

cuedcol = '#3182bd' #blue, use for cued
neutcol = '#bdbdbd' #light-ish grey, use for neutral
diffcol = '#B2DF8A' #for colouring of the bars of difference plots, third distinct colour

wd = 'C:/Users/sammirc/Desktop/phd/wmConfidence_Behaviour'
os.chdir(wd)

np.random.seed(1)


E1dat = pd.read_csv(op.join(wd, 'Experiment1', 'data', 'wmSelection_BehaviouralData_All_Preprocessed.csv'))
subs2use    = [1, 2, 3, 4, 5, 6, 7,    9, 10, 11,     13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
E1dat = E1dat.query('subid in @subs2use')
nsubs = E1dat.subid.unique().size
kappasneut = []
kappascued = []
for isub in E1dat.subid.unique():
    print('- - - - - - working on sub %02d - - - - - -'%isub)
    tmpsub = E1dat.copy().query('subid == @isub')
    tmpsub_neutral = tmpsub.query('cond == "neutral"') #get response error for neutral trials
    tmpsub_cued    = tmpsub.query('cond == "cued"')
    
    B, LL, W = cmm.mixmodel_fit(X = np.deg2rad(tmpsub_neutral.resp.to_numpy()),
                                T = np.deg2rad(tmpsub_neutral.targori.to_numpy()),
                                NT = np.deg2rad(tmpsub_neutral.nontargori.to_numpy()))
    #B holds fitted model params: [kappa, P(target), P(nontarget), P(uniform)]
    kappasneut.append(B[0])
    
    B, LL, W = cmm.mixmodel_fit(X = np.deg2rad(tmpsub_cued.resp.to_numpy()),
                                T = np.deg2rad(tmpsub_cued.targori.to_numpy()),
                                NT = np.deg2rad(tmpsub_cued.nontargori.to_numpy()))
    #B holds fitted model params: [kappa, P(target), P(nontarget), P(uniform)]
    kappascued.append(B[0])

    

kappase_neut = sp.stats.sem(kappasneut, ddof=0) #get standard error of these kappas: 1.7403459686238723
kappamean_neut = np.mean(kappasneut) #get mean of these kappas: 21.326299997413816

kappase_cued = sp.stats.sem(kappascued, ddof=0) #get standard error of these kappas: 1.6771219142958136
kappamean_cued = np.mean(kappascued) #get mean of these kappas: 25.713405876244302

#these can be used to sample individual subject precisions
tmp = wrap90(np.round(np.rad2deg(np.random.vonmises(0, kappamean_neut, 1000)).astype(int)))
tmp2 = wrap90(np.round(np.rad2deg(np.random.vonmises(0, kappamean_cued, 1000)).astype(int)))

vis = True
if vis: #this plots the distributions that are generated by sampling a von mises with the mean fitted kappa of the task data
    fig = plt.figure()
    ax = fig.add_subplot(111)
    # ax.hist(tmp, bins = 30, color = neutcol, alpha = 0.7, label = 'neutral')
    # ax.hist(tmp2, bins = 30, color = cuedcol, alpha = 0.7, label = 'cued')
    sns.histplot(x = tmp,  color = neutcol, ax = ax, stat = 'probability', bins = 30, alpha = 0.6, kde = True)
    sns.histplot(x = tmp2, color = cuedcol, ax = ax, stat = 'probability', bins = 30, alpha = 0.6, kde = True)
    ax.set_xlabel('simulated response error (Â°)')


#%%
pvals = []
tvals = []
nsims = 10000
simbetas = np.empty(shape = (4, nsims))
with progressbar.ProgressBar(max_value = nsims).start() as bar:
    for isim in range(nsims):
        bar.update(isim)
        nsubs = 20
        ntrls = 128 #trials per condition
        alldf = pd.DataFrame()
        for i in range(nsubs): #loop over each participant as we are going to simulate them separately
            # sample the kappa of this participants error distribution from a higher distribution
            # this basically assumes that working memory performance is a distribution across the population and each ppt is a sample from this
            
            
            neutkappa = np.random.normal(loc = kappamean_neut,
                                         scale = kappase_neut,
                                         size = 1)
            # this is actually simulating a single subject kappa
            # aka precision of the vonmises that fits their working memory error distribution
            
            #we can also sample the precision of their error distribution on cued trials
            cuedkappa = np.random.normal(loc = kappamean_cued,
                                         scale = kappase_cued,
                                         size = 1)
            
            #visualise what this parent distribution of concentration parameters looks like (change size to 1000 to do this)
            # fig = plt.figure()
            # ax=fig.add_subplot(111)
            # sns.kdeplot(neutkappa, color = neutcol, fill = neutcol, alpha = 0.6, ax=ax, label = 'neutral')
            # sns.kdeplot(cuedkappa, color = cuedcol, fill = cuedcol, alpha = 0.6, ax=ax, label = 'cued')
            # ax.set_ylabel('density')
            # ax.set_xlabel('concentration parameter (kappa)')
            
            
            #we are also going to sample the standard deviation of their error distribution for cued trials
            #this is going to assume that cueing also leads to better working memory precision, but this improvement amount varies across people
            
            parenterror_neut = wrap90(np.round(np.rad2deg(np.random.vonmises(0, neutkappa, 1000)).astype(int)))
            parenterror_cued = wrap90(np.round(np.rad2deg(np.random.vonmises(0, cuedkappa, 1000)).astype(int)))
            
            vis_parent = False
            if vis_parent: #visualise the error distribution we are going to sample from
                fig = plt.figure(figsize = (10,5))
                ax1 = fig.add_subplot(121)
                ax1.hist(parenterror_neut, color = neutcol, alpha = 0.7, label = 'neutral', bins = 20)
                ax1.hist(parenterror_cued, color = cuedcol, alpha = 0.7, label = 'cued', bins = 20)
                ax1.set_title('error distribution')
                ax2 = fig.add_subplot(122)
                ax2.hist(np.abs(parenterror_neut), color = neutcol, alpha = 0.7, label = 'neutral', bins = 20)
                ax2.hist(np.abs(parenterror_cued), color = cuedcol, alpha = 0.7, label = 'cued', bins = 20)
                ax2.set_title('absolute error distribution')
                fig.legend()
                    
            #sample the parent distribution, with replacement, to simulate the across trial response error distribution
            subjerror_neut = np.random.choice(np.abs(parenterror_neut), size = ntrls, replace = True)
            subjerror_cued = np.random.choice(np.abs(parenterror_cued), size = ntrls, replace = True)
            
            
            
            #scenario 1: no insight into error on a single trial or the error wouldn't be made.
            # instead, perfect insight into across trial performance (distribution)
            # confidence thus sampled from the same distribution as error, so acro trial insight is accurate to the performance distribution
            
            neutconf = np.random.choice(np.abs(parenterror_neut), size = ntrls, replace = True)
            cuedconf = np.random.choice(np.abs(parenterror_cued), size = ntrls, replace = True)
            
            cond = np.concatenate([np.tile(np.array(['neutral']), ntrls), np.tile(np.array(['cued']),ntrls)])
            suberrors = np.concatenate([subjerror_neut, subjerror_cued])
            subconfs = np.concatenate([neutconf, cuedconf])
            
            df = pd.DataFrame(np.array([cond, suberrors, subconfs]).T, columns = ['condition', 'error', 'confidence'])
            df.error = df.error.astype(int)
            df.confidence = df.confidence.astype(int)
            df = df.assign(absrdif = np.abs(df.error))
            df = df.assign(confidence = np.where(df.confidence == 0, 0.001, df.confidence))
            df = df.assign(logconf = np.log(df.confidence))
            df = df.assign(subid = i+1)
            df = df.assign(trlid = np.add(np.arange(len(df)),1))
            
            alldf = pd.concat([alldf, df])
        
        alldf = alldf.assign(conferror = np.subtract(alldf.error, alldf.confidence)) #signed, positive = overconfident, negative = underconfident
        
        # the question: in this simulation, is there an interaction in the linear model showing a more accurate relationship
        # between error and confidence between conditions, purely where the only difference is a more precise error distribution across trials?
        # remember, this simulation assumes no insight on individual trials, but confidence is sampled from the across-trial error distribution
        
        allBetas = pd.DataFrame()
        allfits = []
        df_glms = pd.DataFrame()
        for i in alldf.subid.unique(): #loop over simulate subjects
            subdat = alldf.copy().query('subid == @i').reset_index()
            subdat = subdat.assign(confwidth = np.where(subdat.confidence == 0, 0.1, subdat.confidence)) #cant log a 0, so set this to a similarly small value instead
            subdat = subdat.assign(logcw = np.log(subdat.confwidth))
            
            logcw = subdat.logcw.to_numpy()
            cuetype = subdat.condition.to_numpy()
            cuetype = np.where(cuetype == 'neutral', -1, 1) #set neutral trials to -1, so this is a contrast regressor of cued - neutral
            error = subdat.absrdif.to_numpy()
            error = np.where(error == 0, 0.1, error) #zero error wouldn't get modelled, so set it to similarly small error to be able to include in model
            errXcue = np.multiply(error, cuetype) #code the interaction
            intercept = np.ones(logcw.size)
            
            desMat = np.array([intercept, cuetype, error, errXcue]).T
            
            model = sm.regression.linear_model.OLS(endog = logcw,
                                                          exog = pd.DataFrame(desMat, columns = ['intercept', 'cuetype', 'error', 'cueXerror']))
            # model = sm.regression.linear_model.OLS.from_formula(data = subdat,
            #                                                     formula = 'logcw ~ error*C(cond, Treatment(reference="neutral"))')
            
            fit = model.fit()
            fit.summary()
            betas = pd.DataFrame(fit.params).T
            betas = betas.assign(subid = i)
            
            residuals = fit.resid
            subdat = subdat.assign(residual = residuals,
                                   fittedlogcw = fit.fittedvalues)
            
            df_glms = pd.concat([df_glms, subdat])
            allfits.append(fit)
            allBetas = pd.concat([allBetas, betas])
        
        #random effects analysis of these betas - e.g., are they significantly different from 0 across the group?
        # inference here would be whether a new subject would likely have the same effect
        # inference at subject level, rather than observation (trial) level
        
        allbetas = pd.melt(allBetas, id_vars = 'subid', value_vars = ['intercept', 'cuetype', 'error', 'cueXerror'], var_name = 'regressor', value_name = 'beta')
        allbetas = allbetas.query('regressor != "intercept"') #drop intercept as not that useful to see
        # sns.barplot(allbetas, x = 'regressor', y = 'beta', estimator = 'mean', errorbar = 'se', hue = 'regressor')
        
        
        #t-tests on these betas to test for significant effects across subjects
        # sp.stats.ttest_1samp(allbetas.query('regressor == "cuetype"').beta, popmean = 0, alternative = 'two-sided')
        # # TtestResult(statistic=-1.3502237785309812, pvalue=0.19281002162380942, df=19)
        
        
        # sp.stats.ttest_1samp(allbetas.query('regressor == "error"').beta, popmean = 0, alternative = 'two-sided')
        # # TtestResult(statistic=0.0869227825614776, pvalue=0.9316424309233438, df=19)
        
        
        t, p =  sp.stats.ttest_1samp(allbetas.query('regressor == "cueXerror"').beta, popmean = 0, alternative = 'two-sided')
        pvals.append(p)
        tvals.append(t)
        # # TtestResult(statistic=0.9275781535159155, pvalue=0.36526067103980087, df=19)
        
        
        # #plot these regressor coefficients across subjects
        # plotbetas = allbetas.groupby('regressor', as_index = False).agg({'beta':['mean', sp.stats.sem]})
        # plotbetas.columns = ['regressor', 'meanBeta', 'semBeta']
        # plotbetas = plotbetas.sort_values(by = 'regressor', ascending = False)
        
        # fig = plt.figure(figsize = (4,4))
        # ax = fig.add_subplot(111)
        # ax.bar(x = plotbetas.regressor, height = plotbetas.meanBeta, color = ['#1b9e77', '#d95f02', '#7570b3'], width = 0.5)
        # ax.errorbar(x = plotbetas.regressor, y = plotbetas.meanBeta, yerr = plotbetas.semBeta,
        #             fmt = 'none', ecolor = '#000000', elinewidth = 2, capsize = 7)
        # ax.set_ylabel('beta (AU)')
        # ax.set_axisbelow(True)
        # ax.grid(True, color = '#bdbdbd')
        # fig.tight_layout()

pvals = np.array(pvals)
tvals = np.array(tvals)

# '''
# unless i've made a big code error here, it looks like running a simulation like this can result in *no* effects seen at all

# steps for the simulation:
# - create normal distribution of kappa (concentration parameters) for the population
# - sample the concentration parameter of a von mises distribution from this population distribution, per subject, per condition
#     - essentially randomly samples a subjects performance from a population of possible performances
# - create a parent distribution for this participant, of response errors (1k samples).
# - for the simulated task data, sample randomly (with replacement) from this subject performance parent distribution to get their response error distribution
#     - theoretically this assumes that a participant has a performance distribution and their task data is a subset of this.
#     - if they redid the task, performance wouldn't be the same, but instead would be another independent sample from their personal performance distribution
# - assuming no single trial insight, but insight into variability across trials, sample confidence on each trial from the absolute values of the parent performance distribution
#     - this is done separately for neutral and cued trials. this therefore assumes participants have insight to the effect of the cue on their performance
# - model is then fit separately to each participant, modelling log-confidence as a function of absolute reponse error, cue type, and the interaction of the two
    
# '''
#%%
# the actual interaction significance test that we found (t-test across subjects on the within subject interaction)
# for the random effects analysis (across subject glm rather than LMM):
# t = 2.689, p = 0.0145

# what needs to happen now is really to see what proportion of simulations can give rise to a t-value for this effect (measure of strength and direction)
# that is the same or larger
# or we can construct the 95% confidence interval and see where it lays

if op.exists(op.join(wd, '10ksims_tvalues.npy')):
    tvals = np.loadtxt(op.join(wd, '10ksims_tvalues.npy'))


fig = plt.figure()
ax = fig.add_subplot(111)
ax.hist(tvals, bins = 100)
ax.axvline(x = 2.689, lw = 1, color = 'k', ls = 'dashed')
ax.set_xlabel('t-value')
ax.set_ylabel('count')
ax.set_title('null distribution of t-values')
np.greater(tvals, 2.689).sum()/tvals.size #percentage of simulations with an effect size greater than our observed effect
# (0.0074) <- p-value for the observed t-value under this null distribution of effect sizes

if not op.exists(op.join(wd, '10ksims_tvalues.npy')):
    #save the p- and t-values from the 10k simulations that are run
    np.save(op.join(wd, '10ksims_pvalues.npy'), pvals)
    np.savetxt(op.join(wd, '10ksims_tvalues.npy'), tvals)



#%%


'''
We can run a second simulation that assumes people have insight into their single trial error, but this introspection is a noisy estimate of performance
(e.g. there is some gaussian noise added to the single trial response error)

first want to see if this idea gives qualitatively the similar patterns we find
'''

nsubs = 20
ntrls = 128 #trials per condition
alldf = pd.DataFrame()
for i in range(nsubs): #loop over each participant as we are going to simulate them separately
    # sample the kappa of this participants error distribution from a higher distribution
    # this basically assumes that working memory performance is a distribution across the population and each ppt is a sample from this
    
    
    neutkappa = np.random.normal(loc = kappamean_neut,
                                 scale = kappase_neut,
                                 size = 1)
    # this is actually simulating a single subject kappa
    # aka precision of the vonmises that fits their working memory error distribution
    
    #we can also sample the precision of their error distribution on cued trials
    cuedkappa = np.random.normal(loc = kappamean_cued,
                                 scale = kappase_cued,
                                 size = 1)
    
    
    #we are also going to sample the standard deviation of their error distribution for cued trials
    #this is going to assume that cueing also leads to better working memory precision, but this improvement amount varies across people
    
    parenterror_neut = wrap90(np.round(np.rad2deg(np.random.vonmises(0, neutkappa, 1000)).astype(int)))
    parenterror_cued = wrap90(np.round(np.rad2deg(np.random.vonmises(0, cuedkappa, 1000)).astype(int)))
    
    vis_parent = False
    if vis_parent: #visualise the error distribution we are going to sample from
        fig = plt.figure(figsize = (10,5))
        ax1 = fig.add_subplot(121)
        ax1.hist(parenterror_neut, color = neutcol, alpha = 0.7, label = 'neutral', bins = 20)
        ax1.hist(parenterror_cued, color = cuedcol, alpha = 0.7, label = 'cued', bins = 20)
        ax1.set_title('error distribution')
        ax2 = fig.add_subplot(122)
        ax2.hist(np.abs(parenterror_neut), color = neutcol, alpha = 0.7, label = 'neutral', bins = 20)
        ax2.hist(np.abs(parenterror_cued), color = cuedcol, alpha = 0.7, label = 'cued', bins = 20)
        ax2.set_title('absolute error distribution')
        fig.legend()
        
    subjabserror_parent = np.abs(subjerror_parent.copy())
    
    #sample the parent distribution, with replacement, to simulate the across trial response error distribution
    subjerror_neut = np.random.choice(np.abs(parenterror_neut), size = ntrls, replace = True)
    subjerror_cued = np.random.choice(np.abs(parenterror_cued), size = ntrls, replace = True)
    
    
    
    #scenario 1: no insight into error on a single trial or the error wouldn't be made.
    # instead, perfect insight into across trial performance (distribution)
    # confidence thus sampled from the same distribution as error, so acro trial insight is accurate to the performance distribution
    
    neutconf = np.random.choice(np.abs(parenterror_neut), size = ntrls, replace = True)
    cuedconf = np.random.choice(np.abs(parenterror_cued), size = ntrls, replace = True)
    
    cond = np.concatenate([np.tile(np.array(['neutral']), ntrls), np.tile(np.array(['cued']),ntrls)])
    suberrors = np.concatenate([subjerror_neut, subjerror_cued])
    subconfs = np.concatenate([neutconf, cuedconf])
    
    df = pd.DataFrame(np.array([cond, suberrors, subconfs]).T, columns = ['condition', 'error', 'confidence'])
    df.error = df.error.astype(int)
    df.confidence = df.confidence.astype(int)
    df = df.assign(absrdif = np.abs(df.error))
    df = df.assign(confidence = np.where(df.confidence == 0, 0.001, df.confidence))
    df = df.assign(logconf = np.log(df.confidence))
    df = df.assign(subid = i+1)
    df = df.assign(trlid = np.add(np.arange(len(df)),1))
    
    alldf = pd.concat([alldf, df])

alldf = alldf.assign(conferror = np.subtract(alldf.error, alldf.confidence)) #signed, positive = overconfident, negative = underconfident






#%%
# fig = plt.figure()
# ax = fig.add_subplot(111)
# # ax.hist(error, label = 'raw error', alpha = 0.5)
# ax.hist(np.power(error, 0.5), label = 'sqrt error', alpha = 0.5)
# ax.hist(np.log(error), label = 'log error', alpha = 0.5)
# fig.legend()



